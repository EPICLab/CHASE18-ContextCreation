%!TEX root = main.tex

\section{Methodology}\label{sec:methodology}

\boldification{We observed 5 participants at work. Out of these we chose 3 participants who represent diverse styles of programming. P1 followed Test driven development approach, P4 was a conscientious programmer and S was a tinkerer.}

We conducted a lab study to observe six graduate students with professional programming experience performing a programming task. Their details are presented in Table ~\ref{tab:participant-demographics}.
We provided participants P1-P5 with a prompt requiring planning, designing, and developing software to simulate a traffic intersection. This prompt had previously been used in other studies~\cite{Mangano:2012}. This prompt was easy to understand but nontrivial to implement, allowing us to observe participants' use of context. 

We additionally observed participant P6 working on a real-world problem involving development of his own IDE. We do so to contrast the results of participants who were working on a given prompt, from those that occur when a programmer is working on their on own programming task.

% We observed P6 working on a real-world problem of developing his own IDE. Contrasting the results of P1-P5 with P6 allowed us to differentiate observations that arose due to the specifics of the traffic prompt, from those that occur during typical software development.

\input{tables/participants}

%beyond the domain of ``toy systems''.
Our participants provided a brief, but diverse subset of programming styles; P1 adhered to the Test-Driven Development (TDD) model, and P6 displayed a strong affinity for tinkering~\cite{Beckwith:2006}. %By focusing on these three participants we are able to better present context while programming. 

\boldification{P1 and P4 was given to design(or redesign?? Clarify from Nick) a traffic simulator application that would be used for educational purposes. This style of programming is similar to maintenance or debugging. P6 was working on developing an IDE, which was more exploratory in nature with just some idea about the work and lesser constraints.}

% P6 was not given a prompt because we wanted examine whether our initial observations from P1 to P5 would hold in a unconstrained real-world environment. This allowed us to differentiate observations that arose due to the specifics of the given traffic prompt, and those observations that appear to be universal across programming sessions and goals. The traffic prompt given to P1 to P5 is similar to a prompt use in previous software design studies~\cite{Mangano:2012}, and asks participants to design and implement a simulator for traffic intersections that can accommodate users experimenting with different timing and traffic light signal patterns. This prompt was designed to be easily understandable but difficult to implement, in order to challenge participants and be able to observe their use of context in problem solving. ------Moved up---------

\boldification{We captured their screen for an hour. P1 was asked to think aloud. We used his verbalizations to validate our understanding of context creation, usage and deletion}

We time-boxed the study to one-hour, to prevent participant fatigue~\cite{Easterbrook:2008}. Participants used their preferred development tools and programming language. They were also provided blank paper and given the option to think-aloud. P1 chose to think aloud, which we used to validate our interpretations of P1's actions. We collected a diverse dataset: audio, screen recording, external notes.

% We used this think aloud data to validate our interpretations of participants' actions when programming. Context is a broad notion, and anything---from a paper note's placement to the screen color---can affect context. We obtained diverse data in the form of think aloud audio, screen recording video, facial expressions, and any external notes added to either the blank paper or the printed traffic intersection prompt. 

% We didn't restrict our observation to any medium, as context is a broad notion and anything around a person, from the placement of a paper to the color of the screen, can affect it. We also collected diverse data in the form or think aloud, screen capture, facial expression, video of workspace and any notes or diagrams the participants drew to recognize the subtle hints which may suggest that information was added to the context or obtained from the context. 

% Study sessions lasted time-boxed to a maximum of one hour, and organized as observational lab studies~\cite{Easterbrook:2008}. Participants were allowed to use their preferred development tools during the study, and video capture of their screen was recorded. Paper and pen were provided to participants for design, sketch, and note-taking purposes, and was collected at the conclusion of the study. Only P1 used a think-aloud, which was used to gain a better understanding of their mental model of context while problem solving.

\boldification{We unitized the videos using atlas.ti based on groups of coherent activities. For each unit, we noted the programming activities the participants performed and the artifacts they accesses and in what order. The set of programming activities were large adapted from [Yi wang?s] paper.}

%%% HOW TO EXPLAIN EPISODES WITHOUT EPISODES???????

To analyze the data, the first author unitized the screen recordings and audio transcripts into continuous time segments. Each unit contains a logically consistent group of related interactions that represent a small part of their programming task. %The first author also unitized all audio transcripts.

For each of these units, we identified the programming activity, the artifacts used, and their frequency. Table ~\ref{tab:programming-activities} lists  the 11 programming activities we use in our analysis. The codeset builds on~\cite{Wang:2017}, with slight modifications to fit our task prompt: we changed \texttt{READING QUESTIONS} to \texttt{READING TASK PROMPT} \texttt{(A4)}, and added \texttt{UPDATING DOCUMENTS} \texttt{(A1)}.

Using this codeset, the first and the third authors obtained an Inter-Rater Reliability (IRR) score of 97.14\% and the first and the fourth authors obtained an IRR score of 92\% when coding random segments from the selected participants dataset (which constituted \textasciitilde14\% of the raw data). The remaining 86\% of the data was coded individually by the same authors.

% We identified the artifacts that participants used in these units. We categorized artifacts as web-pages, source code files open and visible on screen, and external notes. Example artifacts include \texttt{intersection.scala} open as a file, and \texttt{stackoverflow.com} visited in the browser. 
%Our codeset is based upon the codes developed by Yi Wang~\cite{Wang:2017}, which include: \texttt{navigation, reading questions, searching, reading search results, processing search results, viewing web resources, coding, run, debugging,} and \texttt{idle}. We do not use the \texttt{accidents} code since determining whether fast switching between artifacts is intentional, and therefore productive, is difficult to appraise from screen capture data. We added \texttt{communication} and \texttt{documentation} to the codeset, due to the nature of the tasks found in the traffic prompt and the think-aloud included in P1.

\boldification{Three of the authors coded the videos, following the IRR process. They obtained a jaccard index of 97.3\%. They also manually noted the order in which participants accessed the artifacts}


